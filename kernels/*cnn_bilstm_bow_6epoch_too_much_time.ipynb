{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle\n",
    "import os, re\n",
    "import nltk\n",
    "BASE_DIR = '../input/'\n",
    "LABELED_TRAIN_DF = BASE_DIR + 'labeled_train_clean_reviews_remove_stopwords.csv'\n",
    "TEST_DF = BASE_DIR + 'test_clean_reviews_remove_stopwords.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 labeled train reviewsand 25000 test reviews\n"
     ]
    }
   ],
   "source": [
    "labeled_train = pd.read_csv(LABELED_TRAIN_DF, header = 0)\n",
    "test = pd.read_csv(TEST_DF, header = 0)\n",
    "labeled_train[\"review\"] = labeled_train[\"review\"].astype(str)\n",
    "test[\"review\"] = test[\"review\"].astype(str)\n",
    "print \"Read %d labeled train reviews\" \\\n",
    "          \"and %d test reviews\" % (labeled_train[\"review\"].size, test[\"review\"].size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data leakage\n",
    "\n",
    "Check if test[\"sentiment\"] is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"sentiment\"] = test[\"id\"].map(lambda x: 1 if int(x.strip('\"').split(\"_\")[1]) >= 5 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean_reviews = labeled_train[\"review\"].tolist()\n",
    "test_clean_reviews = test[\"review\"].tolist()\n",
    "\n",
    "all_clean_reviews = train_clean_reviews + test_clean_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['heard bad things cabin fever almost much heard cultish hype turns first film new impresario eli roth effort iq points dropping film progresses worse movies surely gory ones sure hype meter got high blood count hostel good amount genre fans premise necessarily bad either though kids go cabin week partying come across sick man covered blood panic set fire winds dead water feeds reservoir soon characters succumb flesh eating virus one way another characters either lead college kids including rider strong hero james dibello goofy side bar supporting village folks archetypal point inertia painfully meet fates townspeople get pretty weird seems non sensibly thrown together without many laughs pancakes kid comes nowhere maybe might funnier another movie context rest movie work also young police character even dumber less convincing others family goes dibello following incident possibilities realized roth pumps script common sense window sudden scares frights people hacking blood one another killer dog rambling around either unlike lesser troma horror movies even lesser ones seven zero eight zero sure roth die hard fan make unwatchable also smart explanation virus comes unlike films also really fine idea makes great campy horror times film tries course twice make goofy bloody time like random bunny strong sees gurney even poorly shot lot time albeit contrived style choices ala red tint lens story bowling alley worker simply contains lot illogical scenarios choices made shave legs deadly virus uh huh aims fairly typical ground cup tea power end found actually un exceptional genre territory offend audience sensibilities ala saw swing fence either clever b movie roth also temerity end movie true note hell santa claus bearded convenience store clerk earlier film serves bunch black people could work followed something better dropped altogether could said lot movie c']\n",
      "['film little bad even kind bad horrid know trouble charo shows film must week love boat george kennedy least gamer trying actually right good see martha raye jimmie walker okay slightly better acting airport seven five film better acting airport seven five one thing liked film going lot disaster films plot wise least made attempt subplots really hurts film special effects ugh special ask kind film two truly truly funny fun buddies looking something laugh']\n"
     ]
    }
   ],
   "source": [
    "print(np.random.choice(train_clean_reviews, 1))\n",
    "print(np.random.choice(test_clean_reviews, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false\n",
    "# We vectorize the text corpus by turning each text into a sequence of integers\n",
    "# Each integer is the index of a token in the dictionary\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "MAX_NUM_WORDS_FOR_KERAS_TOKENIZER = 500\n",
    "#\n",
    "# num_words: the maximum number of words to keep, based on frequency.\n",
    "keras_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS_FOR_KERAS_TOKENIZER)\n",
    "#\n",
    "# fit_on_texts accepts a list of strings, a generator of strings or \n",
    "# a list of list of strings. In the last case, it assumes each entry of the lists to be a token.\n",
    "# Here we provide a list of strings.\n",
    "keras_tokenizer.fit_on_texts(train_clean_reviews)\n",
    "word_index = keras_tokenizer.word_index\n",
    "print('Keras Tokenizer found %s unique tokens' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false\n",
    "# each element is <type 'numpy.float64'> \n",
    "embedded_train = keras_tokenizer.texts_to_matrix(train_clean_reviews, mode='count')\n",
    "embedded_test = keras_tokenizer.texts_to_matrix(test_clean_reviews, mode='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false\n",
    "print(len(embedded_train), len(embedded_train[0]))\n",
    "print(len(embedded_test), len(embedded_test[0]))\n",
    "print(embedded_train[0])\n",
    "print(embedded_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false\n",
    "# Reshape data into a 25000 x 5000 x 1 array for feedint 1D Covolution\n",
    "embedded_train = np.reshape(embedded_train, (len(embedded_train), MAX_NUM_WORDS_FOR_KERAS_TOKENIZER, 1))\n",
    "embedded_test = np.reshape(embedded_test, (len(embedded_test), MAX_NUM_WORDS_FOR_KERAS_TOKENIZER, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false\n",
    "print(len(embedded_train), len(embedded_train[0]), len(embedded_train[0][0]))\n",
    "print(len(embedded_test), len(embedded_test[0]), len(embedded_test[0][0]))\n",
    "print(embedded_train[0])\n",
    "print(embedded_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false\n",
    "# Split train_sequences into train and validation. Ratio: 80/20\n",
    "VALIDATION_SPLIT = 0.2\n",
    "np.random.seed(1234)\n",
    "\n",
    "# \n",
    "perm = np.random.permutation(len(embedded_train))\n",
    "index_train = perm[:int(len(embedded_train)*(1-VALIDATION_SPLIT))]\n",
    "index_val = perm[int(len(embedded_train)*(1-VALIDATION_SPLIT)):]\n",
    "\n",
    "x_train = embedded_train[index_train]\n",
    "x_val = embedded_train[index_val]\n",
    "y_train = labeled_train[\"sentiment\"][index_train].tolist()\n",
    "y_val = labeled_train[\"sentiment\"][index_val].tolist()\n",
    "\n",
    "print('Randomly split %d pad sequences for training, %d for validation' % (len(x_train) ,len(x_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false\n",
    "x_test = embedded_test\n",
    "y_test = test[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FEATURES = 5000\n",
    "# Bag of Words\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                         tokenizer = None,    \\\n",
    "                         preprocessor = None, \\\n",
    "                         stop_words = None,   \\\n",
    "                         max_features = MAX_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_features = vectorizer.fit_transform(train_clean_reviews)\n",
    "test_data_features = vectorizer.transform(test_clean_reviews)\n",
    "train_data_features = train_data_features.toarray()\n",
    "test_data_features = test_data_features.toarray()\n",
    "train_data_features = [np.float64(e) for e in train_data_features]\n",
    "test_data_features = [np.float64(e) for e in test_data_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 5000)\n",
      "(25000, 5000)\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data_features), len(train_data_features[0]))\n",
    "print(len(test_data_features), len(test_data_features[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape data into a 25000 x 5000 x 1 array for feedint 1D Covolution\n",
    "train_data_features = np.reshape(train_data_features, (len(train_data_features), MAX_FEATURES, 1))\n",
    "test_data_features = np.reshape(test_data_features, (len(test_data_features), MAX_FEATURES, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 5000, 1)\n",
      "(25000, 5000, 1)\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data_features), len(train_data_features[0]), len(train_data_features[0][0]))\n",
    "print(len(test_data_features), len(test_data_features[0]), len(test_data_features[0][0]))\n",
    "print(train_data_features[0])\n",
    "print(test_data_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_features = np.expand_dims(train_data_features, axis=2)\n",
    "# print(len(train_data_features), len(train_data_features[0]), len(train_data_features[0][0]))\n",
    "# print(len(test_data_features), len(test_data_features[0]), len(test_data_features[0][0]))\n",
    "# print(train_data_features[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly split 20000 pad sequences for training, 5000 for validation\n"
     ]
    }
   ],
   "source": [
    "# Split train_sequences into train and validation. Ratio: 80/20\n",
    "VALIDATION_SPLIT = 0.2\n",
    "np.random.seed(1234)\n",
    "\n",
    "# \n",
    "perm = np.random.permutation(len(train_data_features))\n",
    "index_train = perm[:int(len(train_data_features)*(1-VALIDATION_SPLIT))]\n",
    "index_val = perm[int(len(train_data_features)*(1-VALIDATION_SPLIT)):]\n",
    "\n",
    "x_train = train_data_features[index_train]\n",
    "x_val = train_data_features[index_val]\n",
    "y_train = labeled_train[\"sentiment\"][index_train].tolist()\n",
    "y_val = labeled_train[\"sentiment\"][index_val].tolist()\n",
    "\n",
    "print('Randomly split %d pad sequences for training, %d for validation' % (len(x_train) ,len(x_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test_data_features\n",
    "y_test = test[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 5000, 1)\n",
      "(5000, 5000, 1)\n",
      "(25000, 5000, 1)\n",
      "(20000, 5000, 25000)\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train), len(x_train[0]), len(x_train[0][0]))\n",
    "print(len(x_val), len(x_val[0]), len(x_val[0][0]))\n",
    "print(len(x_test), len(x_test[0]), len(x_test[0][0]))\n",
    "print(len(y_train), len(y_val), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Bidirectional, Conv1D, MaxPooling1D ,GlobalMaxPooling1D\n",
    "from keras.initializers import Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_rnn_model(dropout_cnn=0.5, dropout_rnn=0.2, num_filters=64, kernel_size=2):\n",
    "    model = Sequential()\n",
    "    \n",
    "    LSTM_UNITS = 32\n",
    "    LSTM_DROPOUT = 0.\n",
    "    LSTM_RECCURENT_DROPOUT = 0.\n",
    "\n",
    "    output_layer = Dense(1, activation='sigmoid')\n",
    "    \n",
    "    model.add(Conv1D(filters=num_filters, kernel_size=kernel_size, padding='valid', activation='relu', strides=1\n",
    "                    , input_shape=(5000, 1)))\n",
    "    # Cannot use GlobalMaxPooling since you're feeding it into RNN\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(dropout_cnn))\n",
    "    model.add(Bidirectional(LSTM(LSTM_UNITS, \n",
    "                                 dropout=LSTM_DROPOUT, \n",
    "                                 recurrent_dropout=LSTM_RECCURENT_DROPOUT\n",
    "                                )))\n",
    "    model.add(Dropout(dropout_rnn))\n",
    "    model.add(output_layer)\n",
    "    \n",
    "    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cnn_rnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 4999, 64)          192       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 2499, 64)          0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2499, 32)          2080      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2499, 32)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 64)                16640     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 18,977\n",
      "Trainable params: 18,977\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 2778s 139ms/step - loss: 0.6871 - acc: 0.5328 - val_loss: 0.6733 - val_acc: 0.5888\n"
     ]
    }
   ],
   "source": [
    "# batch_size: number of samples per gradient update\n",
    "history = model.fit(x_train, y_train, batch_size=64, epochs=1, validation_data=[x_val, y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('cnn_bilstm_bow_1epoch.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-d9fa083a4dae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_test_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training_arrays.pyc\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "y_test_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_binary = map(lambda predict: 1 if predict > 0.5 else 0, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The AUC score for CNN-BiLSTM model is : %.5f.\" %roc_auc_score(y_test, y_test_pred_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the test results\n",
    "output = pd.DataFrame(data={\"id\": test[\"id\"], \"sentiment\": y_test_pred_binary})\n",
    "output.to_csv(os.path.join('../', 'output', \"cnn_bilstm_bow_6epoch.csv\"), index=False, quoting=3)\n",
    "print \"Wrote to cnn_bilstm_bow_6epoch.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, color='orange', label='Train')\n",
    "plt.plot(epochs, val_loss, color='green', label='Validation')\n",
    "plt.title('Loss: cnn_bilstm_bow_6epoch')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig('../figures/cnn_bilstm_bow_6epoch_loss.png', dpi=900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc, color='orange', label='Train')\n",
    "plt.plot(epochs, val_acc, color='blue', label='Validation')\n",
    "plt.title('Accuracy: cnn_bilstm_bow_6epoch')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.savefig('../figures/cnn_bilstm_bow_6epoch_accuracy.png', dpi=900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
