{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['word2vec_model_300dim_40minwords_10context', 'sampleSubmission.csv', 'labeledTrainData.tsv', 'test_submission.csv', 'testData.tsv', 'unlabeledTrainData.tsv']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "import os, re\n",
    "import nltk\n",
    "BASE_DIR = '../input/'\n",
    "LABELED_TRAIN_DF = BASE_DIR + 'labeledTrainData.tsv'\n",
    "UNLABELED_TRAIN_DF = BASE_DIR + 'unlabeledTrainData.tsv'\n",
    "TEST_DF = BASE_DIR + 'testData.tsv'\n",
    "print(os.listdir(BASE_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 labeled train reviews, 50000 unlabeled train reviews, and 25000 test reviews\n"
     ]
    }
   ],
   "source": [
    "labeled_train = pd.read_csv(LABELED_TRAIN_DF, header = 0, delimiter = '\\t', quoting=3)\n",
    "unlabeled_train = pd.read_csv(UNLABELED_TRAIN_DF, header = 0, delimiter = '\\t', quoting=3)\n",
    "test = pd.read_csv(TEST_DF, header = 0, delimiter = '\\t', quoting=3)\n",
    "print \"Read %d labeled train reviews, %d unlabeled train reviews, \" \\\n",
    "          \"and %d test reviews\" % (labeled_train[\"review\"].size, unlabeled_train[\"review\"].size, test[\"review\"].size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data leakage\n",
    "\n",
    "Check if test[\"sentiment\"] is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"sentiment\"] = test[\"id\"].map(lambda x: 1 if int(x.strip('\"').split(\"_\")[1]) >= 5 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credits: Kaggle tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def review_to_clean_review(review, remove_numbers=True):\n",
    "    # Function to convert a document to a clean document,\n",
    "    # optionally removing numbers.  Returns a string.\n",
    "    #\n",
    "    # 1. Remove HTML using lxml parser, ranked best by bs4\n",
    "    review_text = BeautifulSoup(review, \"lxml\").get_text()\n",
    "    #\n",
    "    # TODO: Clean the text! stemming?\n",
    "    # https://www.kaggle.com/lystdo/lstm-with-word2vec-embeddings#L92\n",
    "    # https://www.kaggle.com/currie32/the-importance-of-cleaning-text\n",
    "    #  \n",
    "    # 2. Remove non-letters and non-numbers\n",
    "    review_text = re.sub(\"[^a-zA-Z0-9]\", \" \", review_text)\n",
    "    #\n",
    "    # 3. Optionally remove numbers\n",
    "    if remove_numbers:\n",
    "        review_text = re.sub(\"[0-9]\", \" \", review_text)\n",
    "    else:\n",
    "        review_text = review_text.replace('0', ' zero ')\n",
    "        review_text = review_text.replace('1', ' one ')\n",
    "        review_text = review_text.replace('2', ' two ')\n",
    "        review_text = review_text.replace('3', ' three ')\n",
    "        review_text = review_text.replace('4', ' four ')\n",
    "        review_text = review_text.replace('5', ' five ')\n",
    "        review_text = review_text.replace('6', ' six ')\n",
    "        review_text = review_text.replace('7', ' seven ')\n",
    "        review_text = review_text.replace('8', ' eight ')\n",
    "        review_text = review_text.replace('9', ' nine ')\n",
    "    # 6. Return a cleaned string\n",
    "    return(review_text.lower())\n",
    "\n",
    "def review_to_wordlist(review, remove_stopwords=False, remove_numbers=True):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    # 1. Clean review, split it into words\n",
    "    words = review_to_clean_review(review).split()\n",
    "    #\n",
    "    # 2. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 6. Return a list of words\n",
    "    return(words)\n",
    "\n",
    "def review_to_sentences(review, tokenizer, remove_stopwords=False, remove_numbers=True):\n",
    "    # Function to split a review into parsed sentences. Returns a\n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.decode('utf8').strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append(review_to_wordlist(raw_sentence, \\\n",
    "                                                        remove_stopwords, remove_numbers))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the word2vec model vocabulary: 16490\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "EMBEDDING_DIM = 300  # Word vector dimensionality\n",
    "MIN_WORD_COUNT = 40  # Minimum word count. Kaggle set to 40, to avoid attaching too much importance to individual movie titles.\n",
    "NUM_THREADS = 4  # Number of threads to run in parallel\n",
    "CONTEXT = 10  # Context window size\n",
    "DOWNSAMPLING = 1e-3  # Downsample setting for frequent words\n",
    "WORD2VEC_MODEL_FILE = BASE_DIR + \\\n",
    "    \"word2vec_model_\" + \\\n",
    "    str(EMBEDDING_DIM) + \"dim_\" + \\\n",
    "    str(MIN_WORD_COUNT) + \"minwords_\" + \\\n",
    "    str(CONTEXT) + \"context\"\n",
    "\n",
    "word2vec_model = Word2Vec.load(WORD2VEC_MODEL_FILE)\n",
    "\n",
    "# If you don't plan to train the model any further, calling\n",
    "# init_sims will make the model much more memory-efficient.\n",
    "word2vec_model.init_sims(replace=True)\n",
    "\n",
    "print(\"Number of words in the word2vec model vocabulary: %d\" % len(word2vec_model.wv.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train review 0 of 25000\n",
      "Train review 5000 of 25000\n",
      "Train review 10000 of 25000\n",
      "Train review 15000 of 25000\n",
      "Train review 20000 of 25000\n",
      "Test review 0 of 25000\n",
      "Test review 5000 of 25000\n",
      "Test review 10000 of 25000\n",
      "Test review 15000 of 25000\n",
      "Test review 20000 of 25000\n"
     ]
    }
   ],
   "source": [
    "train_clean_reviews = []\n",
    "# Getting clean reviews from training set\n",
    "counter = 0.\n",
    "for review in labeled_train[\"review\"]:\n",
    "    clean_review = review_to_clean_review(review, remove_numbers=False)\n",
    "    train_clean_reviews.append(clean_review)\n",
    "    if counter % 5000. == 0.:\n",
    "        print \"Train review %d of %d\" % (counter, len(labeled_train[\"review\"]))\n",
    "    counter = counter + 1.\n",
    "\n",
    "# train_clean_reviews = map(\n",
    "#     lambda review: review_to_clean_review(review, remove_numbers=False),\n",
    "#     labeled_train[\"review\"])\n",
    "\n",
    "test_clean_reviews = []\n",
    "# Getting clean review from testing set\n",
    "counter = 0.\n",
    "for review in test[\"review\"]:\n",
    "    clean_review = review_to_clean_review(review, remove_numbers=False)\n",
    "    test_clean_reviews.append(clean_review)\n",
    "    if counter % 5000. == 0.:\n",
    "        print \"Test review %d of %d\" % (counter, len(test[\"review\"]))\n",
    "    counter = counter + 1.\n",
    "\n",
    "# test_clean_reviews = map(\n",
    "#     lambda review: review_to_clean_review(review, remove_numbers=False),\n",
    "#     test[\"review\"])\n",
    "\n",
    "all_clean_reviews = train_clean_reviews + test_clean_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(train_clean_reviews[0:2])\n",
    "# print(test_clean_reviews[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras Tokenizer found 101376 unique tokens\n"
     ]
    }
   ],
   "source": [
    "# We vectorize the text corpus by turning each text into a sequence of integers\n",
    "# Each integer is the index of a token in the dictionary\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "MAX_NUM_WORDS_FOR_KERAS_TOKENIZER = 200000\n",
    "#\n",
    "# num_words: the maximum number of words to keep, based on frequency.\n",
    "keras_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS_FOR_KERAS_TOKENIZER)\n",
    "#\n",
    "# fit_on_texts accepts a list of strings, a generator of strings or \n",
    "# a list of list of strings. In the last case, it assumes each entry of the lists to be a token.\n",
    "# Here we provide a list of strings.\n",
    "keras_tokenizer.fit_on_texts(all_clean_reviews)\n",
    "word_index = keras_tokenizer.word_index\n",
    "print('Keras Tokenizer found %s unique tokens' % len(word_index))\n",
    "#\n",
    "# texts_to_sequences transforms each text in texts to a sequence of integers.\n",
    "train_sequences = keras_tokenizer.texts_to_sequences(train_clean_reviews)\n",
    "test_sequences = keras_tokenizer.texts_to_sequences(test_clean_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We pad all text sequences to the same length.\n",
    "# By default zeros are padded at the front.\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Set max length for each review sequence.\n",
    "MAX_SEQUENCE_LENGTH_FOR_KERAS_RNN = 500\n",
    "\n",
    "train_pad_sequences = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH_FOR_KERAS_RNN)\n",
    "test_pad_sequences = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH_FOR_KERAS_RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null word embeddings: 84920\n"
     ]
    }
   ],
   "source": [
    "# Prepare word embedding matrix\n",
    "\n",
    "# Choose the smaller number of the two as column length of the matrix\n",
    "num_words = min(MAX_NUM_WORDS_FOR_KERAS_TOKENIZER, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec_model.wv.vocab:\n",
    "        embedding_matrix[i] = word2vec_model.wv.get_vector(word)\n",
    "# Null word embeddings are words that don't exist in the embedding matrix\n",
    "# and are therefore represented as zero vectors.\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly split 20000 pad sequences for training, 5000 for validation\n"
     ]
    }
   ],
   "source": [
    "# Split train_sequences into train and validation. Ratio: 80/20\n",
    "VALIDATION_SPLIT = 0.2\n",
    "np.random.seed(1234)\n",
    "\n",
    "# \n",
    "perm = np.random.permutation(len(train_sequences))\n",
    "index_train = perm[:int(len(train_sequences)*(1-VALIDATION_SPLIT))]\n",
    "index_val = perm[int(len(train_sequences)*(1-VALIDATION_SPLIT)):]\n",
    "\n",
    "x_train = train_pad_sequences[index_train]\n",
    "x_val = train_pad_sequences[index_val]\n",
    "y_train = labeled_train[\"sentiment\"][index_train].tolist()\n",
    "y_val = labeled_train[\"sentiment\"][index_val].tolist()\n",
    "\n",
    "print('Randomly split %d pad sequences for training, %d for validation' % (len(x_train) ,len(x_val)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test_pad_sequences\n",
    "y_test = test[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.initializers import Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    # https://towardsdatascience.com/rnn-training-tips-and-tricks-2bf687e67527\n",
    "    LSTM_UNITS = 100\n",
    "    LSTM_DROPOUT = 0.\n",
    "    LSTM_RECCURENT_DROPOUT = 0.\n",
    "    \n",
    "    # Embedding layer.\n",
    "    # First two parameters are input_dim and output_dim.\n",
    "    # weights, trainable are passed into **kwargs.\n",
    "    # They are properties of the abstract base Layer class from which all layers inherit.\n",
    "    embedding_layer = Embedding(\n",
    "            num_words,\n",
    "            EMBEDDING_DIM,\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=MAX_SEQUENCE_LENGTH_FOR_KERAS_RNN,\n",
    "            trainable=False)\n",
    "    lstm_layer = LSTM(LSTM_UNITS, dropout=LSTM_DROPOUT, \n",
    "                      recurrent_dropout=LSTM_RECCURENT_DROPOUT)\n",
    "    output_layer = Dense(1, activation='sigmoid')\n",
    "    \n",
    "    model.add(embedding_layer)\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(lstm_layer)\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(output_layer)\n",
    "    \n",
    "    # Choose adam optimizer, binary crossentropy, ['accuracy'] metric.\n",
    "    # https://www.dlology.com/blog/quick-notes-on-how-to-choose-optimizer-in-keras/\n",
    "    # https://www.dlology.com/blog/how-to-choose-last-layer-activation-and-loss-function/\n",
    "    # https://keras.io/getting-started/sequential-model-guide/\n",
    "    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = rnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 300)          30413100  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500, 300)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               160400    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 30,573,601\n",
      "Trainable params: 160,501\n",
      "Non-trainable params: 30,413,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/6\n",
      "20000/20000 [==============================] - 571s 29ms/step - loss: 0.5202 - acc: 0.7515 - val_loss: 0.4303 - val_acc: 0.8082\n",
      "Epoch 2/6\n",
      "20000/20000 [==============================] - 597s 30ms/step - loss: 0.4218 - acc: 0.8207 - val_loss: 0.3707 - val_acc: 0.8472\n",
      "Epoch 3/6\n",
      "20000/20000 [==============================] - 569s 28ms/step - loss: 0.3789 - acc: 0.8407 - val_loss: 0.3890 - val_acc: 0.8426\n",
      "Epoch 4/6\n",
      "20000/20000 [==============================] - 562s 28ms/step - loss: 0.4096 - acc: 0.8242 - val_loss: 0.4363 - val_acc: 0.8164\n",
      "Epoch 5/6\n",
      "20000/20000 [==============================] - 568s 28ms/step - loss: 0.3696 - acc: 0.8471 - val_loss: 0.3242 - val_acc: 0.8684\n",
      "Epoch 6/6\n",
      "20000/20000 [==============================] - 562s 28ms/step - loss: 0.3218 - acc: 0.8686 - val_loss: 0.3265 - val_acc: 0.8652\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4aef8dbbd0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch_size: number of samples per gradient update\n",
    "lstm_model.fit(x_train, y_train, batch_size=32, epochs=6, validation_data=[x_val, y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_lstm = lstm_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC score for LSTM model is : 0.9405.\n"
     ]
    }
   ],
   "source": [
    "print(\"The AUC score for LSTM model is : %.4f.\" %roc_auc_score(y_test, y_test_pred_lstm))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
